{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at finding a corpus of words related to Financial articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import demjson\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from collections import Counter\n",
    "from datetime import datetime, date\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "best_performers = ['NFLX', 'CSX', 'NRG', 'SWKS', 'URI']\n",
    "worst_performers = ['ENDP', 'BMY', 'SIG', 'GPRO', 'CYH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    \"\"\" Turns a date string into a valid datetime object\"\"\"\n",
    "    date_str_regex = re.compile(\"[\\w]{3} [\\d]{1,2}, [\\d]{4}\")\n",
    "    hours_ago_regex = re.compile(\"[\\d]{1,2} [\\w]+ ago\")\n",
    "    if date_str_regex.match(date_str):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%b %d, %Y\")\n",
    "        except ValueError:\n",
    "                pass\n",
    "    elif hours_ago_regex.match(date_str):\n",
    "        try:\n",
    "            return datetime.combine(date.today(), datetime.min.time())\n",
    "        except:\n",
    "            pass\n",
    "    raise ValueError(\"couldn't parse date string: {0}\".format(date_str))\n",
    "\n",
    "def _make_keys_verbose(article_dict):\n",
    "    old_to_new = {\n",
    "        \"a\": \"articles\", \"d\": \"date\", \"s\": \"source\", \"t\": \"title\",\n",
    "        \"tt\": \"titleId\", \"u\": \"url\", \"sp\": \"openingSentence\"\n",
    "    }\n",
    "    for old_key in article_dict:\n",
    "        if old_key in old_to_new:\n",
    "            new_key = old_to_new[old_key]\n",
    "            article_dict[new_key] = article_dict.pop(old_key)\n",
    "        \n",
    "def get_articles(symbol, num_articles=500):\n",
    "    payload = {\n",
    "        \"output\": \"json\", \"q\": symbol, \"num\": num_articles,\"start\": 0}\n",
    "    try:\n",
    "        resp = requests.get(\"http://www.google.com/finance/company_news?\", params=payload)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        return\n",
    "    # need demjson's decode, json data is invalid for pythons native decoder\n",
    "    clusters = demjson.decode(resp.text)['clusters']\n",
    "    articles = []\n",
    "    for cluster in clusters:\n",
    "        if \"a\" in cluster:\n",
    "            for article in cluster[\"a\"]:\n",
    "                _make_keys_verbose(article)\n",
    "                article['date'] = parse_date(article['date'])\n",
    "                articles.append(article)\n",
    "    return articles\n",
    "\n",
    "def tokenize_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    return [word.strip().replace('\\n', '') for word in words]\n",
    "\n",
    "def download_articles(urls):\n",
    "    article_texts = []\n",
    "    for url in urls:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        if 'wsj.com' in url or not article.is_downloaded:\n",
    "            continue\n",
    "        article.parse()\n",
    "        article_texts.append(article.text)\n",
    "    return article_texts\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    return [ word for word in words if word not in set(stopwords.words('english')) ]\n",
    "\n",
    "def word_freqs(articles):\n",
    "    \"\"\" pass in List[ List[words] ]\"\"\"\n",
    "    word_freqs = Counter()\n",
    "    # accumulate the counter\n",
    "    for article in articles:\n",
    "        word_freqs += Counter(article) # Counter supports `+`\n",
    "    return word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use google finance API to get articles\n",
    "best_dict = {ticker: get_articles(ticker, 200) \n",
    "             for ticker in best_performers}\n",
    "\n",
    "worst_dict = {ticker: get_articles(ticker, 200)\n",
    "              for ticker in worst_performers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get urls to pass to analysis functions\n",
    "cutoff_date = datetime(2017,1,1)\n",
    "best_performer_urls = [d['url'] \n",
    "                       for ticker in best_dict \n",
    "                       for d in best_dict[ticker] if d['date'] >= cutoff_date]\n",
    "worst_performer_urls = [d['url'] \n",
    "                        for ticker in worst_dict \n",
    "                        for d in worst_dict[ticker] if d['date'] >= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# downloading each article, takes a long time\n",
    "best_performer_articles = download_articles(best_performer_urls[:25])\n",
    "worst_performer_articles = download_articles(worst_performer_urls[:25])\n",
    "\n",
    "#%store best_performer_articles\n",
    "#%store worst_performer_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "best_performer_article_words = [ filter_stopwords( tokenize_words(article) ) \n",
    "                                 for article in best_performer_articles ]\n",
    "\n",
    "worst_performer_article_words = [ filter_stopwords( tokenize_words(article) )\n",
    "                                  for article in worst_performer_articles ]\n",
    "\n",
    "best_performer_word_freqs = word_freqs(best_performer_article_words)\n",
    "worst_performer_word_freqs = word_freqs(worst_performer_article_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "worst_performer_word_freqs.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "worst_performer_word_freqs = word_freqs(worst_performer_articles)\n",
    "best_perfomer_word_freqs = word_freqs(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "adjTags = ['JJ', 'JJR', 'JJS']\n",
    "wordsAndTags = nltk.pos_tag(worst_performer_word_freqs.keys())\n",
    "worst_performer_adjs = copy.deepcopy(worst_performer_word_freqs)\n",
    "adjs = { t[0] for t in wordsAndTags if t[1] in adjTags }\n",
    "for word in list(worst_performer_adjs.keys()):\n",
    "    if word not in adjs:\n",
    "        del worst_performer_adjs[word]\n",
    "worst_performer_adjs.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "worst_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "freqs.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:vestview]",
   "language": "python",
   "name": "conda-env-vestview-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
